{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/burakbeyazit/grad-project/blob/main/Bitirme_Projem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P57qd3UoU95g"
      },
      "source": [
        "\n",
        "Data Preprocessing...\n",
        "\n",
        "Dropping rows which contains NaN or empty values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "collapsed": true,
        "id": "bFlwgTgGtoRW",
        "outputId": "9fff86b4-cd65-4e8b-9a3d-fdf99f168dce"
      },
      "outputs": [
        {
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-60d69eac340d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Flight_Datas/Flight_Points_Actual_20220601_20220630.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Ön temizlik: eksik verileri sil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Flight_Datas/Flight_Points_Actual_20220601_20220630.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Ön temizlik: eksik verileri sil\n",
        "df_clean = df.dropna().copy()\n",
        "\n",
        "# Koordinat düzeltme fonksiyonu\n",
        "def fix_coordinate(value):\n",
        "    try:\n",
        "        if pd.isna(value):\n",
        "            return None\n",
        "        if isinstance(value, str) and value.startswith(\".\"):\n",
        "            value = \"0\" + value\n",
        "        value = float(value)\n",
        "        if abs(value) > 180:\n",
        "            value /= 100000\n",
        "        return round(value, 6)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Latitude ve Longitude sütunlarını temizle\n",
        "df_clean[\"Latitude\"] = df_clean[\"Latitude\"].apply(fix_coordinate)\n",
        "df_clean[\"Longitude\"] = df_clean[\"Longitude\"].apply(fix_coordinate)\n",
        "\n",
        "# Hatalı koordinatları tamamen sil\n",
        "df_clean = df_clean.dropna(subset=[\"Latitude\", \"Longitude\"])\n",
        "\n",
        "df_clean.to_csv(file_path, index=False)\n",
        "\n",
        "print(\"Dosya eksiksiz ve temiz olarak güncellendi.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install netCDF4 h5netcdf xarray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z53AI2GpsqTy",
        "outputId": "134620ab-02e6-40be-f1f6-f17483b115d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting netCDF4\n",
            "  Downloading netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting h5netcdf\n",
            "  Downloading h5netcdf-1.6.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting xarray\n",
            "  Downloading xarray-2025.4.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from netCDF4) (2025.4.26)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from netCDF4) (2.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from h5netcdf) (3.13.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from h5netcdf) (25.0)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from xarray) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray) (1.17.0)\n",
            "Downloading netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5netcdf-1.6.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xarray-2025.4.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cftime, netCDF4, h5netcdf, xarray\n",
            "Successfully installed cftime-1.6.4.post1 h5netcdf-1.6.1 netCDF4-1.7.2 xarray-2025.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "filed_csv  = \"/content/drive/MyDrive/Flight_Datas/Flight_Points_Filed_20220601_20220630.csv\"\n",
        "actual_csv = \"/content/drive/MyDrive/Flight_Datas/Flight_Points_Actual_20220601_20220630.csv\"\n",
        "nc_path    = \"/content/drive/MyDrive/Flight_Datas/data_stream-oper_stepType-instant.nc\"\n",
        "flights_meta = '/content/drive/MyDrive/Flight_Datas/Flights_20220601_20220630.csv'\n",
        "\n",
        "df_meta   = pd.read_csv(flights_meta)\n",
        "\n",
        "df_filed  = pd.read_csv(filed_csv)\n",
        "df_actual = pd.read_csv(actual_csv)"
      ],
      "metadata": {
        "id": "UoI8SckcadYx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "cutoff = pd.Timestamp(\"2022-06-07 23:00:00\")\n",
        "grid_res = 0.5\n",
        "def preprocess(df):\n",
        "    df = df.copy()\n",
        "    df[\"Time Over\"] = pd.to_datetime(df[\"Time Over\"], dayfirst=True)\n",
        "    df = df[df[\"Time Over\"] <= cutoff]\n",
        "    df[\"TimeHour\"] = df[\"Time Over\"].dt.floor(\"h\")\n",
        "    df[\"Lat_grid\"] = (df[\"Latitude\"] / grid_res).round() * grid_res\n",
        "    df[\"Lon_grid\"] = (df[\"Longitude\"] / grid_res).round() * grid_res\n",
        "    return df\n",
        "\n",
        "df_filed = preprocess(df_filed)\n",
        "df_actual = preprocess(df_actual)"
      ],
      "metadata": {
        "id": "Y8QgWU5qb2RF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "51xdeE2voG2g",
        "outputId": "ca7964fa-9631-40ef-c496-34dcc149cd06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "⛅ Weather cache: 100%|██████████| 1415476/1415476 [19:45<00:00, 1194.02it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'index'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'index'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b6d00564a08c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# === 5) DataFrame’e çevir ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mwc_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweather_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mwc_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Lat_grid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Lon_grid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TimeHour\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mwc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'index'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Dosya yolları ===\n",
        "filed_csv  = \"/content/drive/MyDrive/Flight_Datas/Flight_Points_Filed_20220601_20220630.csv\"\n",
        "actual_csv = \"/content/drive/MyDrive/Flight_Datas/Flight_Points_Actual_20220601_20220630.csv\"\n",
        "nc_path    = \"/content/drive/MyDrive/Flight_Datas/data_stream-oper_stepType-instant.nc\"\n",
        "\n",
        "# === Ayarlar ===\n",
        "cutoff = pd.Timestamp(\"2022-06-07 23:00:00\")\n",
        "grid_res = 0.5\n",
        "\n",
        "\n",
        "# === 2) Benzersiz noktalar ===\n",
        "combined = pd.concat([\n",
        "    df_filed[[\"Lat_grid\", \"Lon_grid\", \"TimeHour\"]],\n",
        "    df_actual[[\"Lat_grid\", \"Lon_grid\", \"TimeHour\"]]\n",
        "])\n",
        "unique_pts = combined.drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "# === 3) NetCDF aç ===\n",
        "ds = xr.open_dataset(nc_path).load()\n",
        "\n",
        "# === 4) Hava durumu verisi cache ===\n",
        "weather_cache = {}\n",
        "for _, row in tqdm(unique_pts.iterrows(), total=len(unique_pts), desc=\"⛅ Weather cache\"):\n",
        "    lat, lon, time = row[\"Lat_grid\"], row[\"Lon_grid\"], row[\"TimeHour\"]\n",
        "    key = (lat, lon, time)\n",
        "    try:\n",
        "        pt = ds.sel(valid_time=np.datetime64(time), latitude=lat, longitude=lon, method=\"nearest\")\n",
        "        weather_cache[key] = {\n",
        "            \"Temp_C\": float(pt[\"t2m\"].values) - 273.15,\n",
        "            \"DewPoint_C\": float(pt[\"d2m\"].values) - 273.15,\n",
        "            \"U_Wind_10m\": float(pt[\"u10\"].values),\n",
        "            \"V_Wind_10m\": float(pt[\"v10\"].values),\n",
        "            \"MSL_Pressure_hPa\": float(pt[\"msl\"].values) / 100,\n",
        "            \"Surface_Pressure_hPa\": float(pt[\"sp\"].values) / 100,\n",
        "            \"CloudBase_m\": float(pt[\"cbh\"].values),\n",
        "            \"LowCloud\": float(pt[\"lcc\"].values),\n",
        "            \"TotalCloud\": float(pt[\"tcc\"].values),\n",
        "        }\n",
        "    except:\n",
        "        weather_cache[key] = {k: None for k in [\n",
        "            \"Temp_C\", \"DewPoint_C\", \"U_Wind_10m\", \"V_Wind_10m\",\n",
        "            \"MSL_Pressure_hPa\", \"Surface_Pressure_hPa\",\n",
        "            \"CloudBase_m\", \"LowCloud\", \"TotalCloud\"\n",
        "        ]}\n",
        "\n",
        "# === 5) DataFrame’e çevir ===\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wc_df = pd.DataFrame.from_dict(weather_cache, orient=\"index\")\n",
        "wc_df.index = pd.MultiIndex.from_tuples(wc_df.index, names=[\"Lat_grid\", \"Lon_grid\", \"TimeHour\"])\n",
        "wc_df.reset_index(inplace=True)\n",
        "\n",
        "\n",
        "# === 6) Merge ile dosyalara ekle ===\n",
        "df_filed  = df_filed.merge(wc_df, on=[\"Lat_grid\", \"Lon_grid\", \"TimeHour\"], how=\"left\")\n",
        "df_actual = df_actual.merge(wc_df, on=[\"Lat_grid\", \"Lon_grid\", \"TimeHour\"], how=\"left\")\n",
        "\n",
        "# === 7) Temizlik ve kayıt ===\n",
        "df_filed.drop(columns=[\"Lat_grid\", \"Lon_grid\", \"TimeHour\"], inplace=True)\n",
        "df_actual.drop(columns=[\"Lat_grid\", \"Lon_grid\", \"TimeHour\"], inplace=True)\n",
        "\n",
        "df_filed.to_csv(filed_csv, index=False)\n",
        "df_actual.to_csv(actual_csv, index=False)\n",
        "\n",
        "print(\"✅ Filed ve Actual verilerine hava durumu başarıyla eklendi.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5x2zbwNjffy",
        "outputId": "9450ded2-e05d-479f-d7bd-ea178a5535fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Filed ve Actual verilerine hava durumu başarıyla eklendi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItlaZ3q1VOiv"
      },
      "source": [
        "Adding Destination and Source point informations on route logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV91rTZmV9t1"
      },
      "source": [
        "This project compares planned and actual flight paths using a machine learning model.\n",
        "We trained a Random Forest model to predict how much the actual flight deviates from the planned route.\n",
        "We used 300 flights and split the data into 80% training and 20% testing.\n",
        "The model works well, with an R² score of 0.959 and low prediction error.\n",
        "We also visualized the real vs predicted deviations, and most of the predictions are very close to the actual values."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_meta.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FhY7vw7p23V",
        "outputId": "28328409-527d-4ecd-a6a0-f295e80ffcd1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ECTRL ID', 'ADEP', 'ADEP Latitude', 'ADEP Longitude', 'ADES', 'ADES Latitude', 'ADES Longitude', 'FILED OFF BLOCK TIME', 'FILED ARRIVAL TIME', 'ACTUAL OFF BLOCK TIME', 'ACTUAL ARRIVAL TIME', 'AC Type', 'AC Operator', 'AC Registration', 'ICAO Flight Type', 'STATFOR Market Segment', 'Requested FL', 'Actual Distance Flown (nm)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Meta’dan sadece ECTRL ID, ADEP, ADES sütunlarını al\n",
        "meta_subset = df_meta[[\"ECTRL ID\", \"ADEP\", \"ADES\"]]\n",
        "\n",
        "# 2) actual ve filed’e merge ederken suffix vermeye gerek yok\n",
        "df_actual = df_actual.merge(\n",
        "    meta_subset,\n",
        "    on=\"ECTRL ID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "df_filed = df_filed.merge(\n",
        "    meta_subset,\n",
        "    on=\"ECTRL ID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# 3) Eğer önceden yaptığın merge’lerde _x/_y kalmışsa onları temizle\n",
        "#    (aşağıdaki iki satırı buna göre düzenle)\n",
        "for df in (df_actual, df_filed):\n",
        "    # eğer _x/_y kolonları varsa drop et\n",
        "    for col in [\"ADEP_x\",\"ADES_x\",\"ADEP_y\",\"ADES_y\"]:\n",
        "        if col in df.columns:\n",
        "            df.drop(columns=col, inplace=True)\n",
        "\n",
        "# 4) Artık df_actual ve df_filed’da sadece tek bir ADEP, ADES sütunu var\n",
        "print(df_actual.head())\n",
        "print(df_filed.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-E32uV4yLSc",
        "outputId": "ad42dcfb-a2d7-4aec-be58-1992f9b4f256"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ECTRL ID  Sequence Number           Time Over  Flight Level  Latitude  \\\n",
            "0  252119478                0 2022-05-31 23:50:00             0  51.88500   \n",
            "1  252119478                1 2022-06-01 00:02:00             0  51.88500   \n",
            "2  252119478                2 2022-06-01 00:03:25            29  51.83500   \n",
            "3  252119478                3 2022-06-01 00:04:15            50  51.85833   \n",
            "4  252119478                4 2022-06-01 00:04:52            66  51.90750   \n",
            "\n",
            "   Longitude     Temp_C  DewPoint_C  U_Wind_10m  V_Wind_10m  MSL_Pressure_hPa  \\\n",
            "0    0.23500  16.471338   15.322229   -0.496323    1.753113         1023.8725   \n",
            "1    0.23500  16.471338   15.322229   -0.496323    1.753113         1023.8725   \n",
            "2    0.16056  16.471338   15.322229   -0.496323    1.753113         1023.8725   \n",
            "3    0.09750  16.471338   15.322229   -0.496323    1.753113         1023.8725   \n",
            "4    0.08195  16.471338   15.322229   -0.496323    1.753113         1023.8725   \n",
            "\n",
            "   Surface_Pressure_hPa  CloudBase_m  LowCloud  TotalCloud  ADEP  ADES  \n",
            "0           1015.527109  2963.904297       0.0     0.98999  EGSS  EGAA  \n",
            "1           1015.527109  2963.904297       0.0     0.98999  EGSS  EGAA  \n",
            "2           1015.527109  2963.904297       0.0     0.98999  EGSS  EGAA  \n",
            "3           1015.527109  2963.904297       0.0     0.98999  EGSS  EGAA  \n",
            "4           1015.527109  2963.904297       0.0     0.98999  EGSS  EGAA  \n",
            "    ECTRL ID  Sequence Number           Time Over  Flight Level  Latitude  \\\n",
            "0  252119478                0 2022-06-01 00:00:00             0  51.88500   \n",
            "1  252119478                1 2022-06-01 00:15:00             0  51.88500   \n",
            "2  252119478                2 2022-06-01 00:16:15            36  51.83500   \n",
            "3  252119478                3 2022-06-01 00:16:58            40  51.85833   \n",
            "4  252119478                4 2022-06-01 00:17:51            40  51.90750   \n",
            "\n",
            "   Longitude     Temp_C  DewPoint_C  U_Wind_10m  V_Wind_10m  MSL_Pressure_hPa  \\\n",
            "0    0.23500  16.471338   15.322229   -0.496323    1.753113         1023.8725   \n",
            "1    0.23500  16.471338   15.322229   -0.496323    1.753113         1023.8725   \n",
            "2    0.16056  16.471338   15.322229   -0.496323    1.753113         1023.8725   \n",
            "3    0.09750  16.471338   15.322229   -0.496323    1.753113         1023.8725   \n",
            "4    0.08195  16.471338   15.322229   -0.496323    1.753113         1023.8725   \n",
            "\n",
            "   Surface_Pressure_hPa  CloudBase_m  LowCloud  TotalCloud  ADEP  ADES  \n",
            "0           1015.527109  2963.904297       0.0     0.98999  EGSS  EGAA  \n",
            "1           1015.527109  2963.904297       0.0     0.98999  EGSS  EGAA  \n",
            "2           1015.527109  2963.904297       0.0     0.98999  EGSS  EGAA  \n",
            "3           1015.527109  2963.904297       0.0     0.98999  EGSS  EGAA  \n",
            "4           1015.527109  2963.904297       0.0     0.98999  EGSS  EGAA  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "Z-AdVWV6KgZG",
        "outputId": "5103fd15-f083-49e6-9da5-63331eb02adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtre sonrası unique ECTRL ID sayısı: 185550\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-73de38ba18cc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     results[name] = {\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0maccept_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositive\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         X, y = validate_data(\n\u001b[0m\u001b[1;32m    602\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1) Tarih sütunlarını datetime tipine çevirme ---\n",
        "df_filed[\"Time Over\"]  = pd.to_datetime(df_filed[\"Time Over\"])\n",
        "df_actual[\"Time Over\"] = pd.to_datetime(df_actual[\"Time Over\"])\n",
        "# Eğer df_meta'da da tarih varsa:\n",
        "df_meta[\"ACTUAL OFF BLOCK TIME\"]   = pd.to_datetime(df_meta[\"ACTUAL OFF BLOCK TIME\"])\n",
        "\n",
        "# --- 2) Kesme tarihi ile filtreleme ---\n",
        "cutoff = pd.Timestamp(\"2022-06-07 23:00:00\")\n",
        "filed_cut  = df_filed[df_filed[\"Time Over\"] <= cutoff]\n",
        "actual_cut = df_actual[df_actual[\"Time Over\"] <= cutoff]\n",
        "meta_cut   = df_meta[df_meta[\"ACTUAL OFF BLOCK TIME\"] <= cutoff]\n",
        "\n",
        "# 3) Filtre sonrası unique ECTRL ID sayısını gösterme\n",
        "unique_meta_ids = meta_cut[\"ECTRL ID\"].nunique()\n",
        "print(f\"Filtre sonrası unique ECTRL ID sayısı: {unique_meta_ids}\")\n",
        "\n",
        "# --- 4) Filed ve Actual verilerini birleştirme ---\n",
        "merged = actual_cut.merge(\n",
        "    filed_cut,\n",
        "    on=[\"ECTRL ID\", \"Sequence Number\"],\n",
        "    suffixes=(\"_actual\", \"_filed\"),\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "\n",
        "# --- 6) Sapma hesaplama (Haversine) ---\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = sin(dlat/2)**2 + cos(lat1)*cos(lat2)*sin(dlon/2)**2\n",
        "    return 6371 * 2 * asin(sqrt(a))\n",
        "\n",
        "merged[\"deviation_km\"] = merged.apply(\n",
        "    lambda r: haversine(\n",
        "        r[\"Latitude_actual\"], r[\"Longitude_actual\"],\n",
        "        r[\"Latitude_filed\"],   r[\"Longitude_filed\"]\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# --- 7) One-hot encoding ---\n",
        "onehot = pd.get_dummies(merged[[\"ADEP\",\"ADES\"]], prefix=[\"ADEP\",\"ADES\"])\n",
        "merged = pd.concat([merged, onehot], axis=1)\n",
        "\n",
        "# --- 8) Özellik ve hedef tanımları ---\n",
        "feature_cols = [\n",
        "    \"Latitude_filed\", \"Longitude_filed\", \"Flight Level_filed\",\n",
        "    # Meteo değişkenleri (filed zamanı)\n",
        "    \"Temp_C_filed\", \"DewPoint_C_filed\", \"U_Wind_10m_filed\", \"V_Wind_10m_filed\",\n",
        "    \"MSL_Pressure_hPa_filed\", \"Surface_Pressure_hPa_filed\",\n",
        "    \"CloudBase_m_filed\", \"LowCloud_filed\", \"TotalCloud_filed\"\n",
        "] + list(onehot.columns)\n",
        "\n",
        "X = merged[feature_cols]\n",
        "y = merged[\"deviation_km\"]\n",
        "\n",
        "# --- 9) Train/Test split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- 10) Modelleri tanımlayıp eğitme ve karşılaştırma ---\n",
        "models = {\n",
        "    \"LinearRegression\": LinearRegression(),\n",
        "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "results = {}\n",
        "\n",
        "for name, mdl in models.items():\n",
        "    mdl.fit(X_train, y_train)\n",
        "    preds = mdl.predict(X_test)\n",
        "    results[name] = {\n",
        "        \"MAE\": mean_absolute_error(y_test, preds),\n",
        "        \"RMSE\": np.sqrt(mean_squared_error(y_test, preds)),\n",
        "        \"R2\": r2_score(y_test, preds)\n",
        "    }\n",
        "    # Scatter plot\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(y_test, preds, alpha=0.5)\n",
        "    lim = max(y_test.max(), preds.max())\n",
        "    plt.plot([0, lim], [0, lim], 'r--')\n",
        "    plt.title(f\"{name}: Gerçek vs Tahmin Sapma\")\n",
        "    plt.xlabel(\"Gerçek Sapma (km)\")\n",
        "    plt.ylabel(\"Tahmin Sapma (km)\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Performansları tablo şeklinde gösterme\n",
        "metrics_df = pd.DataFrame(results).T.rename_axis(\"Model\").reset_index()\n",
        "print(metrics_df)\n",
        "\n",
        "# --- 11) En çok sapma yaratan 10 uçuşu gösterme ---\n",
        "# Uçuş bazında maksimum sapmayı hesapla\n",
        "flight_dev = merged.groupby(\"ECTRL ID\")[\"deviation_km\"].max().reset_index()\n",
        "# En yüksek 10 sapma\n",
        "top10 = flight_dev.nlargest(10, \"deviation_km\")\n",
        "print(\"\\nEn çok sapma yaratan 10 uçuş (ECTRL ID ve sapma km):\")\n",
        "print(top10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Zaman sıralaması (aynı uçuş içindeki akış bozulmasın)\n",
        "merged.sort_values([\"ECTRL ID\", \"Time Over_actual\"], inplace=True)\n",
        "\n",
        "# 2) Aynı ECTRL ID grubunda önceki ve sonraki geçerli değerlerle impute et\n",
        "merged[\"CloudBase_m_filed\"] = (\n",
        "    merged\n",
        "      .groupby(\"ECTRL ID\")[\"CloudBase_m_filed\"]\n",
        "      .apply(lambda s: s.ffill().bfill())\n",
        "      .reset_index(level=0, drop=True)\n",
        ")\n",
        "\n",
        "# 3) Son durumu kontrol et\n",
        "print(\"CloudBase_m_filed hâlâ NaN sayısı:\", merged[\"CloudBase_m_filed\"].isna().sum())\n",
        "\n",
        "# Artık merged DataFrame’in içinde CloudBase_m_filed eksiksiz güncellenmiş durumda.\n",
        "# İsterseniz bu haliyle df_filed veya df_actual’a da geri dağıtabilirsiniz,\n",
        "# ama genelde bu merged üzerinden devam etmek yeterli.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_taFmhLB29EG",
        "outputId": "9ddc3cb2-b68d-4df0-c209-1ff13c91811e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CloudBase_m_filed hâlâ NaN sayısı: 48454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Kalan NaN’ları 0 ile doldur\n",
        "merged[\"CloudBase_m_filed\"].fillna(0, inplace=True)\n",
        "\n",
        "# 5) Kontrol\n",
        "print(\"İşlem sonrası CloudBase_m_filed hâlâ NaN sayısı:\", merged[\"CloudBase_m_filed\"].isna().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7l9_KTR4Bzg",
        "outputId": "14072bc0-f4de-4c79-df05-3e039a2a2664"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "İşlem sonrası CloudBase_m_filed hâlâ NaN sayısı: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-19e3aaed91a9>:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  merged[\"CloudBase_m_filed\"].fillna(0, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 10) Modelleri tanımlayıp eğitme ve karşılaştırma ---\n",
        "models = {\n",
        "    \"LinearRegression\": LinearRegression(),\n",
        "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "results = {}\n",
        "\n",
        "for name, mdl in models.items():\n",
        "    mdl.fit(X_train, y_train)\n",
        "    preds = mdl.predict(X_test)\n",
        "    results[name] = {\n",
        "        \"MAE\": mean_absolute_error(y_test, preds),\n",
        "        \"RMSE\": np.sqrt(mean_squared_error(y_test, preds)),\n",
        "        \"R2\": r2_score(y_test, preds)\n",
        "    }\n",
        "    # Scatter plot\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(y_test, preds, alpha=0.5)\n",
        "    lim = max(y_test.max(), preds.max())\n",
        "    plt.plot([0, lim], [0, lim], 'r--')\n",
        "    plt.title(f\"{name}: Gerçek vs Tahmin Sapma\")\n",
        "    plt.xlabel(\"Gerçek Sapma (km)\")\n",
        "    plt.ylabel(\"Tahmin Sapma (km)\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Performansları tablo şeklinde gösterme\n",
        "metrics_df = pd.DataFrame(results).T.rename_axis(\"Model\").reset_index()\n",
        "print(metrics_df)\n",
        "\n",
        "# --- 11) En çok sapma yaratan 10 uçuşu gösterme ---\n",
        "# Uçuş bazında maksimum sapmayı hesapla\n",
        "flight_dev = merged.groupby(\"ECTRL ID\")[\"deviation_km\"].max().reset_index()\n",
        "# En yüksek 10 sapma\n",
        "top10 = flight_dev.nlargest(10, \"deviation_km\")\n",
        "print(\"\\nEn çok sapma yaratan 10 uçuş (ECTRL ID ve sapma km):\")\n",
        "print(top10)"
      ],
      "metadata": {
        "id": "p4f35jcA4D5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Her sütundaki eksik oranı (%)\n",
        "missing_pct = merged.isna().mean() * 100\n",
        "\n",
        "# Sadece eksik değeri olanları göster\n",
        "print(missing_pct[missing_pct > 0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX6UpF2Y1UFU",
        "outputId": "779b0ab1-c4d9-4ad2-fabb-674b69bbe53d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CloudBase_m_actual    21.105962\n",
            "CloudBase_m_filed     21.056479\n",
            "ADEP Latitude          0.081460\n",
            "ADEP Longitude         0.081460\n",
            "ADES Latitude          0.093637\n",
            "ADES Longitude         0.093637\n",
            "AC Registration        0.045139\n",
            "dtype: float64\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "mount_file_id": "1ZwL-IlrFdpRUFSd2bYh8rGiHMIIjd-bf",
      "authorship_tag": "ABX9TyNDF/VbEAkUJkKSmg4cbVaw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}